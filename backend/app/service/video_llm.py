import json
from google import genai
from google.genai import types
from app.config import settings
from app.core.rag_engine import rag_engine

class VideoLLMService:
    def __init__(self):
        self.client = genai.Client(
            api_key=settings.API_KEY,
            http_options={"base_url": settings.GOOGLE_GENAI_BASE_URL}
        )
        # ä½¿ç”¨ gemini-2.0-flash æˆ– gemini-1.5-pro
        self.model_name = "gemini-2.0-flash" 

    def chat(self, message: str, history: list = None, model_name: str = "gemini-2.0-flash", stream: bool = False):
        """
        æ™®é€š AI å¯¹è¯åŠŸèƒ½
        Role: åº·å¤è®­ç»ƒæŒ‡å¯¼
        æ”¯æŒæµå¼è¾“å‡º
        """
        print(f"[INFO] ðŸ’¬ æ”¶åˆ°ç”¨æˆ·æ¶ˆæ¯: {message[:50]}{'...' if len(message) > 50 else ''}")
        print(f"[INFO] ðŸ¤– ä½¿ç”¨æ¨¡åž‹: {model_name}")
        print(f"[INFO] ðŸ“š åŽ†å²æ¶ˆæ¯æ•°: {len(history) if history else 0}")
        print(f"[INFO] âš¡ æµå¼è¾“å‡º: {'æ˜¯' if stream else 'å¦'}")
        
        system_instruction = """
        ä½ æ˜¯ä¸€åä¸“ä¸šçš„åº·å¤è®­ç»ƒæŒ‡å¯¼æ•™ç»ƒã€‚ä½ çš„èŒè´£æ˜¯ï¼š
        1. ä»¥ä¸“ä¸šã€äº²åˆ‡ã€é¼“åŠ±çš„å£å»ä¸Žç”¨æˆ·äº¤æµã€‚
        2. è§£ç­”ç”¨æˆ·å…³äºŽåº·å¤åŠ¨ä½œã€èº«ä½“æ¢å¤ã€è¿åŠ¨å¥åº·æ–¹é¢çš„é—®é¢˜ã€‚
        3. åœ¨å›žç­”æ—¶ï¼Œå°½é‡æ¸…æ™°æ˜Žç¡®ï¼Œé¿å…è¿‡äºŽæ™¦æ¶©çš„åŒ»å­¦æœ¯è¯­ï¼Œç¡®ä¿ç”¨æˆ·èƒ½å¬æ‡‚ã€‚
        4. é€‚å½“åœ°ä¸Žç”¨æˆ·äº’åŠ¨ï¼Œä¾‹å¦‚è¯¢é—®ä»–ä»¬ç›®å‰çš„ç–¼ç—›ç¨‹åº¦ã€åº·å¤è¿›å±•æˆ–å…·ä½“éœ€æ±‚ã€‚
        5. å¦‚æžœç”¨æˆ·ä¸Šä¼ äº†è§†é¢‘ï¼ˆé€šè¿‡ä¸Šä¸‹æ–‡å¾—çŸ¥ï¼‰ï¼Œè¯·ç»“åˆè§†é¢‘å†…å®¹è¿›è¡ŒæŒ‡å¯¼ã€‚
        """
        
        # æž„å»ºå¯¹è¯åŽ†å²
        contents = []
        if history:
            for msg in history:
                role = "user" if msg["role"] == "user" else "model"
                # è¿‡æ»¤æŽ‰éžæ–‡æœ¬å†…å®¹ï¼ˆå¦‚è§†é¢‘å ä½ç¬¦ï¼‰
                if isinstance(msg["content"], str) and not msg["content"].startswith("[ç³»ç»Ÿ"):
                     contents.append(types.Content(role=role, parts=[types.Part(text=msg["content"])]))
        
        contents.append(types.Content(role="user", parts=[types.Part(text=message)]))

        print(f"[INFO] ðŸš€ å¼€å§‹è°ƒç”¨ Gemini API...")
        
        if stream:
            # æµå¼è¿”å›žç”Ÿæˆå™¨
            print(f"[INFO] ðŸ“¡ ä½¿ç”¨æµå¼æ¨¡å¼ç”Ÿæˆå›žå¤")
            return self.client.models.generate_content_stream(
                model=model_name,
                contents=contents,
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                )
            )
        else:
            # éžæµå¼è¿”å›žå®Œæ•´æ–‡æœ¬
            response = self.client.models.generate_content(
                model=model_name,
                contents=contents,
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction
                )
            )
            response_text = response.text
            print(f"[INFO] âœ… API è°ƒç”¨æˆåŠŸï¼Œå“åº”é•¿åº¦: {len(response_text)} å­—ç¬¦")
            print(f"[INFO] ðŸ“„ å“åº”é¢„è§ˆ: {response_text[:100]}{'...' if len(response_text) > 100 else ''}")
            return response_text

    def process_video_pipeline(self, video_path: str, user_prompt: str = None, progress_callback=None, model_name: str = "gemini-3-pro-preview"):
        def log(msg):
            print(f"[INFO] {msg}")
            if progress_callback:
                progress_callback(msg)

        # 1. è¯»å–è§†é¢‘äºŒè¿›åˆ¶
        log("æ­£åœ¨è¯»å–è§†é¢‘æ–‡ä»¶...")
        with open(video_path, "rb") as f:
            video_bytes = f.read()

        # 2. è§†é¢‘ç†è§£ (Video -> Text)
        log(f"æ­£åœ¨è°ƒç”¨æ¨¡åž‹ ({model_name}) è¿›è¡Œè§†é¢‘ç†è§£ï¼Œåˆ†æžåº·å¤åŠ¨ä½œ...")
        
        base_analyze_prompt = """
        ä½ æ˜¯ä¸€åä¸“ä¸šçš„åº·å¤è®­ç»ƒä¸“å®¶ã€‚è¯·è¯¦ç»†åˆ†æžè¿™ä¸ªè§†é¢‘ä¸­çš„åº·å¤åŠ¨ä½œã€‚
        1. è¯†åˆ«å…·ä½“çš„åŠ¨ä½œåç§°ã€‚
        2. åˆ†æ­¥éª¤æè¿°åŠ¨ä½œç»†èŠ‚ã€èº«ä½“å§¿æ€ã€å‘åŠ›ç‚¹ä»¥åŠæ³¨æ„äº‹é¡¹ã€‚
        3. æŒ‡å‡ºåŠ¨ä½œä¸­å¯èƒ½å­˜åœ¨çš„é”™è¯¯æˆ–éœ€è¦æ”¹è¿›çš„åœ°æ–¹ï¼ˆå¦‚æžœæœ‰ï¼‰ã€‚
        4. è¯­æ°”è¦ä¸“ä¸šä¸”å…·æœ‰æŒ‡å¯¼æ€§ã€‚
        """
        
        if user_prompt:
            analyze_prompt = f"{base_analyze_prompt}\n\nç”¨æˆ·ç‰¹åˆ«å…³æ³¨ç‚¹/é¢å¤–æŒ‡ä»¤ï¼š{user_prompt}\nè¯·åœ¨åˆ†æžæ—¶é‡ç‚¹ç»“åˆç”¨æˆ·çš„æŒ‡ä»¤è¿›è¡Œå›žç­”ã€‚"
        else:
            analyze_prompt = base_analyze_prompt
        
        response = self.client.models.generate_content(
            model=model_name,
            contents=types.Content(
                parts=[
                    types.Part(
                        inline_data=types.Blob(
                            data=video_bytes,
                            mime_type="video/mp4"
                        )
                    ),
                    types.Part(text=analyze_prompt)
                ]
            )
        )
        raw_description = response.text
        log(f"åˆæ­¥è¯†åˆ«å®Œæˆ: {raw_description[:30]}...")

        # 3. RAG æ£€ç´¢å¢žå¼º
        log("æ­£åœ¨æŸ¥è¯¢ RAG çŸ¥è¯†åº“ï¼ŒèŽ·å–ç›¸å…³ä¸“ä¸šå»ºè®®...")
        rag_context = rag_engine.query(raw_description)

        # 4. ç”Ÿæˆæœ€ç»ˆè„šæœ¬ (Text + Context -> JSON)
        log("æ­£åœ¨ç”Ÿæˆæœ€ç»ˆçš„æ•™å­¦æ¼”ç¤ºè„šæœ¬...")
        
        final_prompt_template = f"""
        ä½ æ˜¯ä¸€ååº·å¤è®­ç»ƒæŒ‡å¯¼ã€‚åŸºäºŽè§†é¢‘çš„åŠ¨ä½œæè¿°å’Œæ£€ç´¢åˆ°çš„ä¸“ä¸šçŸ¥è¯†ï¼Œç”Ÿæˆä¸€ä»½ç®€æ´çš„æ•™å­¦è§†é¢‘è„šæœ¬ã€‚

        [åŠ¨ä½œæè¿°]: {raw_description}
        [ä¸“ä¸šçŸ¥è¯†åº“]: {rag_context}
        """
        
        if user_prompt:
            final_prompt_template += f"\n[ç”¨æˆ·é¢å¤–æŒ‡ä»¤]: {user_prompt}\nè¯·ç¡®ä¿ç”Ÿæˆçš„è„šæœ¬å†…å®¹å›žåº”äº†ç”¨æˆ·çš„æŒ‡ä»¤ã€‚"

        final_prompt_template += """
        
        è¯·è¾“å‡ºä¸¥æ ¼çš„ JSON æ ¼å¼ï¼ˆä¸è¦åŒ…å« ```json æ ‡è®°ï¼‰ã€‚ç”Ÿæˆ5-8é¡µå¹»ç¯ç‰‡ï¼Œæ¯é¡µå†…å®¹è¦è¯¦ç»†å……å®žï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
        {
            "slides": [
                {
                    "title": "å¹»ç¯ç‰‡æ ‡é¢˜",
                    "bullets": ["è¦ç‚¹1ï¼ˆè¯¦ç»†æè¿°ï¼‰", "è¦ç‚¹2ï¼ˆè¯¦ç»†æè¿°ï¼‰", "è¦ç‚¹3ï¼ˆè¯¦ç»†æè¿°ï¼‰", "è¦ç‚¹4ï¼ˆè¯¦ç»†æè¿°ï¼‰"],
                    "narration": "è¯¦ç»†çš„è®²è§£è¯ï¼Œæ¯é¡µ3-5å¥è¯ï¼Œéœ€è¦åŒ…å«ï¼šåŠ¨ä½œè¦é¢†ã€æ³¨æ„äº‹é¡¹ã€å¸¸è§é”™è¯¯ç­‰ã€‚è®²è§£è¦ç”ŸåŠ¨ã€ä¸“ä¸šä¸”æ˜“æ‡‚ã€‚"
                }
            ]
        }
        
        è¦æ±‚ï¼š
        1. æ¯é¡µå¹»ç¯ç‰‡è‡³å°‘åŒ…å«3-5ä¸ªè¦ç‚¹
        2. æ¯ä¸ªè¦ç‚¹è¦è¯¦ç»†å…·ä½“ï¼Œä¸è¦å¤ªç®€ç•¥
        3. è®²è§£è¯è¦å……åˆ†å±•å¼€ï¼Œæ¯é¡µè‡³å°‘50-100å­—
        4. å†…å®¹è¦è¦†ç›–ï¼šåŠ¨ä½œå‡†å¤‡ã€æ‰§è¡Œæ­¥éª¤ã€æ³¨æ„äº‹é¡¹ã€å¸¸è§é”™è¯¯ã€æ•ˆæžœè¯´æ˜Žç­‰å¤šä¸ªç»´åº¦
        5. è¯­è¨€è¦ä¸“ä¸šä½†é€šä¿—æ˜“æ‡‚ï¼Œé€‚åˆæ‚£è€…ç†è§£
        """
        
        final_prompt = final_prompt_template

        try:
            log("æ­£åœ¨è°ƒç”¨æ¨¡åž‹ç”Ÿæˆè„šæœ¬...")
            script_response = self.client.models.generate_content(
                model=model_name,
                contents=types.Content(parts=[types.Part(text=final_prompt)]),
                config=types.GenerateContentConfig(
                    response_mime_type="application/json",
                    temperature=0.7,
                    max_output_tokens=4096  # å¢žåŠ åˆ°4096ä»¥æ”¯æŒæ›´é•¿å†…å®¹
                )
            )
            log("è„šæœ¬ç”ŸæˆæˆåŠŸï¼Œæ­£åœ¨è§£æž JSON...")
            script_data = json.loads(script_response.text)
            log(f"è„šæœ¬è§£æžå®Œæˆï¼Œå…± {len(script_data.get('slides', []))} é¡µå¹»ç¯ç‰‡")
            return script_data, raw_description
        except json.JSONDecodeError as e:
            log(f"JSON è§£æžé”™è¯¯: {str(e)}")
            log(f"åŽŸå§‹å“åº”: {script_response.text[:200]}")
            # è¿”å›žä¸€ä¸ªé»˜è®¤çš„è„šæœ¬
            default_script = {
                "slides": [
                    {
                        "title": "åº·å¤åŠ¨ä½œåˆ†æž",
                        "bullets": ["è§†é¢‘å·²åˆ†æžå®Œæˆ", "è¯·æŸ¥çœ‹è¯¦ç»†æ–‡å­—åˆ†æž"],
                        "narration": "è§†é¢‘åˆ†æžå·²å®Œæˆï¼Œè¯¦ç»†ä¿¡æ¯è¯·å‚è€ƒæ–‡å­—æè¿°ã€‚"
                    }
                ]
            }
            return default_script, raw_description
        except Exception as e:
            log(f"ç”Ÿæˆè„šæœ¬æ—¶å‡ºé”™: {str(e)}")
            # è¿”å›žä¸€ä¸ªé»˜è®¤çš„è„šæœ¬
            default_script = {
                "slides": [
                    {
                        "title": "åº·å¤åŠ¨ä½œåˆ†æž",
                        "bullets": ["è§†é¢‘å·²åˆ†æžå®Œæˆ", "è¯·æŸ¥çœ‹è¯¦ç»†æ–‡å­—åˆ†æž"],
                        "narration": "è§†é¢‘åˆ†æžå·²å®Œæˆï¼Œè¯¦ç»†ä¿¡æ¯è¯·å‚è€ƒæ–‡å­—æè¿°ã€‚"
                    }
                ]
            }
            return default_script, raw_description

    def generate_session_title(self, first_message: str, model_name: str = "gemini-2.0-flash"):
        """
        åŸºäºŽå¯¹è¯çš„ç¬¬ä¸€æ¡æ¶ˆæ¯ï¼Œç”Ÿæˆç®€æ´çš„ä¼šè¯æ ‡é¢˜
        """
        prompt = f"""
        è¯·ä¸ºä»¥ä¸‹å¯¹è¯ç”Ÿæˆä¸€ä¸ªç®€æ´çš„æ ‡é¢˜ï¼ˆä¸è¶…è¿‡15ä¸ªå­—ï¼‰ã€‚
        æ ‡é¢˜è¦èƒ½æ¦‚æ‹¬å¯¹è¯çš„ä¸»è¦å†…å®¹ï¼Œä½¿ç”¨ä¸“ä¸šä½†æ˜“æ‡‚çš„è¯­è¨€ã€‚
        
        å¯¹è¯å†…å®¹ï¼š{first_message}
        
        åªè¿”å›žæ ‡é¢˜æ–‡æœ¬ï¼Œä¸è¦åŒ…å«å¼•å·ã€æ ‡ç‚¹æˆ–å…¶ä»–è¯´æ˜Žã€‚
        ç¤ºä¾‹æ ¼å¼ï¼š
        - è…°æ¤Žåº·å¤åŠ¨ä½œåˆ†æž
        - è†å…³èŠ‚ç–¼ç—›å’¨è¯¢
        - æ ¸å¿ƒè‚Œç¾¤è®­ç»ƒæŒ‡å¯¼
        """
        
        try:
            response = self.client.models.generate_content(
                model=model_name,
                contents=types.Content(parts=[types.Part(text=prompt)]),
                config=types.GenerateContentConfig(
                    temperature=0.3,  # è¾ƒä½Žçš„æ¸©åº¦ä»¥èŽ·å¾—æ›´ç¨³å®šçš„ç»“æžœ
                    max_output_tokens=50
                )
            )
            title = response.text.strip()
            # ç§»é™¤å¯èƒ½çš„å¼•å·
            title = title.strip('"').strip("'").strip('ã€Šã€‹')
            # é™åˆ¶é•¿åº¦
            if len(title) > 20:
                title = title[:20] + "..."
            return title
        except Exception as e:
            print(f"[WARN] ç”Ÿæˆæ ‡é¢˜å¤±è´¥: {e}")
            # è¿”å›žé»˜è®¤æ ‡é¢˜
            return first_message[:15] + ("..." if len(first_message) > 15 else "")

video_llm = VideoLLMService()